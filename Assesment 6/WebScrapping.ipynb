{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "source=requests.get(\"https://coreyms.com/\").text\n",
    "#for html code retrival\n",
    "soup= BeautifulSoup(source,'lxml')\n",
    "#to understand the retrived code and parse it as an html code using lmxl parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding all the elements with the tag article and limiting the number of retrivals to 5 articles only\n",
    "articles=soup.find_all('article',limit=5)\n",
    "counter=0\n",
    "#creating and opening a csv file to output the final result in it and making the first row of it as data headers\n",
    "csvfile= open('E:\\Work\\AiCourse\\Assesments\\Assesment 6\\ScrapeResult.csv','w',encoding=\"utf-8\")\n",
    "csvw=csv.writer(csvfile)\n",
    "csvw.writerow([\"Title\",\"Time&Author\",\"Paragraph\",\"Link\",\"Filed Under\",\"Tags\"])\n",
    "#looping on the five articles to scrap the required data\n",
    "for i in range(5):\n",
    "#getting the anchor elements which are inside the h2 tag and taking the text of them [article title]\n",
    "    a=articles[i].h2.a.text\n",
    "#getting the paragraph elements which are inside the div tag and taking the text of them [article description]\n",
    "    p=articles[i].div.p.text\n",
    "#getting the text of the time and date element and article author\n",
    "    tauthor=articles[i].p.time.text+\" by \"+articles[i].p.a.span.text\n",
    "#retriving all the spans having class entry categories\n",
    "    temp=articles[i].find('span', {'class' : 'entry-categories'})\n",
    "#try and catch to avoid having articles without entry categories and to handle this case by adding no value\n",
    "    try:\n",
    "        span=temp.text\n",
    "    except Exception as e:\n",
    "        span=\"No Value\"\n",
    "#retriving all the spans having class entry tags\n",
    "    temp1=articles[i].find('span', {'class' : 'entry-tags'})\n",
    "#try and catch to avoid having articles without entry tags and to handle this case by adding no value\n",
    "    try:\n",
    "        span1=temp1.text\n",
    "    except Exception as e:\n",
    "        span1=\"No Value\"\n",
    "#try and catch to avoid having articles without videos and to handle this case by adding no value\n",
    "    try:\n",
    "#finding the iframe element in the article and getting the link inside its src attribute\n",
    "        vid=articles[i].find('iframe',class_= 'youtube-player')['src']\n",
    "#splitting the link according to the / and ? mark into list to seperate the id of the video\n",
    "        vid_id=vid.split('/')\n",
    "        video=vid_id[4].split('?')\n",
    "        c=video[0]\n",
    "#creating a direct link to youtube instead of the embeded link\n",
    "        link=f'https://youtube.com/watch?v={c}'\n",
    "    except Exception as e:\n",
    "        link=\"No Value\"\n",
    "#writing the result of each article scarp to the csv file\n",
    "    csvw.writerow([a,tauthor,p,link,span,span1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
